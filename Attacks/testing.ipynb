{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/softwares/anaconda/anaconda3/envs/adv_time/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.7193, -0.4033, -0.5966],\n",
      "        [ 0.1820, -0.8567,  1.1006]])\n",
      "torch.Size([1, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/softwares/anaconda/anaconda3/envs/adv_time/lib/python3.7/site-packages/torch/_tensor.py:1104: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/build/aten/src/ATen/core/TensorBody.h:475.)\n",
      "  return self._grad\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'float' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2992179/1962027349.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m## update x\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m0.1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'float' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "l = nn.Linear(3,4)\n",
    "x = torch.randn(2,3)\n",
    "print(x)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "y = torch.randn([1,4])\n",
    "\n",
    "\n",
    "x.requires_grad = True\n",
    "x = x[0,:].unsqueeze(0)\n",
    "y_hat = l(x)\n",
    "print(y_hat.shape)\n",
    "\n",
    "loss = loss(y_hat,y)\n",
    "loss.backward(retain_graph=True)\n",
    "## update x\n",
    "x.data = x.data - 0.1 * x.grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7260, -0.4671, -0.5566]], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathways.vit_models.prompt_models_196_cat_prompt import resPromptClip\n",
    "\n",
    "model = resPromptClip(\n",
    "                input_resolution = 224,\n",
    "                patch_size = 16,\n",
    "                width = 768,\n",
    "                layers = 12,\n",
    "                heads = 768 // 64,\n",
    "                output_dim = 512,\n",
    "                num_prompts=1,\n",
    "                num_classes = 1000,\n",
    "                actual_num_classes = 101,\n",
    "                num_frames=8,\n",
    "                attention_type=\"joint_space_time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['model.class_embedding', 'model.positional_embedding', 'model.proj', 'model.resPrompt_token', 'model.conv1.weight', 'model.ln_pre.weight', 'model.ln_pre.bias', 'model.transformer.resblocks.0.attn.in_proj_weight', 'model.transformer.resblocks.0.attn.in_proj_bias', 'model.transformer.resblocks.0.attn.out_proj.weight', 'model.transformer.resblocks.0.attn.out_proj.bias', 'model.transformer.resblocks.0.ln_1.weight', 'model.transformer.resblocks.0.ln_1.bias', 'model.transformer.resblocks.0.mlp.c_fc.weight', 'model.transformer.resblocks.0.mlp.c_fc.bias', 'model.transformer.resblocks.0.mlp.c_proj.weight', 'model.transformer.resblocks.0.mlp.c_proj.bias', 'model.transformer.resblocks.0.ln_2.weight', 'model.transformer.resblocks.0.ln_2.bias', 'model.transformer.resblocks.1.attn.in_proj_weight', 'model.transformer.resblocks.1.attn.in_proj_bias', 'model.transformer.resblocks.1.attn.out_proj.weight', 'model.transformer.resblocks.1.attn.out_proj.bias', 'model.transformer.resblocks.1.ln_1.weight', 'model.transformer.resblocks.1.ln_1.bias', 'model.transformer.resblocks.1.mlp.c_fc.weight', 'model.transformer.resblocks.1.mlp.c_fc.bias', 'model.transformer.resblocks.1.mlp.c_proj.weight', 'model.transformer.resblocks.1.mlp.c_proj.bias', 'model.transformer.resblocks.1.ln_2.weight', 'model.transformer.resblocks.1.ln_2.bias', 'model.transformer.resblocks.2.attn.in_proj_weight', 'model.transformer.resblocks.2.attn.in_proj_bias', 'model.transformer.resblocks.2.attn.out_proj.weight', 'model.transformer.resblocks.2.attn.out_proj.bias', 'model.transformer.resblocks.2.ln_1.weight', 'model.transformer.resblocks.2.ln_1.bias', 'model.transformer.resblocks.2.mlp.c_fc.weight', 'model.transformer.resblocks.2.mlp.c_fc.bias', 'model.transformer.resblocks.2.mlp.c_proj.weight', 'model.transformer.resblocks.2.mlp.c_proj.bias', 'model.transformer.resblocks.2.ln_2.weight', 'model.transformer.resblocks.2.ln_2.bias', 'model.transformer.resblocks.3.attn.in_proj_weight', 'model.transformer.resblocks.3.attn.in_proj_bias', 'model.transformer.resblocks.3.attn.out_proj.weight', 'model.transformer.resblocks.3.attn.out_proj.bias', 'model.transformer.resblocks.3.ln_1.weight', 'model.transformer.resblocks.3.ln_1.bias', 'model.transformer.resblocks.3.mlp.c_fc.weight', 'model.transformer.resblocks.3.mlp.c_fc.bias', 'model.transformer.resblocks.3.mlp.c_proj.weight', 'model.transformer.resblocks.3.mlp.c_proj.bias', 'model.transformer.resblocks.3.ln_2.weight', 'model.transformer.resblocks.3.ln_2.bias', 'model.transformer.resblocks.4.attn.in_proj_weight', 'model.transformer.resblocks.4.attn.in_proj_bias', 'model.transformer.resblocks.4.attn.out_proj.weight', 'model.transformer.resblocks.4.attn.out_proj.bias', 'model.transformer.resblocks.4.ln_1.weight', 'model.transformer.resblocks.4.ln_1.bias', 'model.transformer.resblocks.4.mlp.c_fc.weight', 'model.transformer.resblocks.4.mlp.c_fc.bias', 'model.transformer.resblocks.4.mlp.c_proj.weight', 'model.transformer.resblocks.4.mlp.c_proj.bias', 'model.transformer.resblocks.4.ln_2.weight', 'model.transformer.resblocks.4.ln_2.bias', 'model.transformer.resblocks.5.attn.in_proj_weight', 'model.transformer.resblocks.5.attn.in_proj_bias', 'model.transformer.resblocks.5.attn.out_proj.weight', 'model.transformer.resblocks.5.attn.out_proj.bias', 'model.transformer.resblocks.5.ln_1.weight', 'model.transformer.resblocks.5.ln_1.bias', 'model.transformer.resblocks.5.mlp.c_fc.weight', 'model.transformer.resblocks.5.mlp.c_fc.bias', 'model.transformer.resblocks.5.mlp.c_proj.weight', 'model.transformer.resblocks.5.mlp.c_proj.bias', 'model.transformer.resblocks.5.ln_2.weight', 'model.transformer.resblocks.5.ln_2.bias', 'model.transformer.resblocks.6.attn.in_proj_weight', 'model.transformer.resblocks.6.attn.in_proj_bias', 'model.transformer.resblocks.6.attn.out_proj.weight', 'model.transformer.resblocks.6.attn.out_proj.bias', 'model.transformer.resblocks.6.ln_1.weight', 'model.transformer.resblocks.6.ln_1.bias', 'model.transformer.resblocks.6.mlp.c_fc.weight', 'model.transformer.resblocks.6.mlp.c_fc.bias', 'model.transformer.resblocks.6.mlp.c_proj.weight', 'model.transformer.resblocks.6.mlp.c_proj.bias', 'model.transformer.resblocks.6.ln_2.weight', 'model.transformer.resblocks.6.ln_2.bias', 'model.transformer.resblocks.7.attn.in_proj_weight', 'model.transformer.resblocks.7.attn.in_proj_bias', 'model.transformer.resblocks.7.attn.out_proj.weight', 'model.transformer.resblocks.7.attn.out_proj.bias', 'model.transformer.resblocks.7.ln_1.weight', 'model.transformer.resblocks.7.ln_1.bias', 'model.transformer.resblocks.7.mlp.c_fc.weight', 'model.transformer.resblocks.7.mlp.c_fc.bias', 'model.transformer.resblocks.7.mlp.c_proj.weight', 'model.transformer.resblocks.7.mlp.c_proj.bias', 'model.transformer.resblocks.7.ln_2.weight', 'model.transformer.resblocks.7.ln_2.bias', 'model.transformer.resblocks.8.attn.in_proj_weight', 'model.transformer.resblocks.8.attn.in_proj_bias', 'model.transformer.resblocks.8.attn.out_proj.weight', 'model.transformer.resblocks.8.attn.out_proj.bias', 'model.transformer.resblocks.8.ln_1.weight', 'model.transformer.resblocks.8.ln_1.bias', 'model.transformer.resblocks.8.mlp.c_fc.weight', 'model.transformer.resblocks.8.mlp.c_fc.bias', 'model.transformer.resblocks.8.mlp.c_proj.weight', 'model.transformer.resblocks.8.mlp.c_proj.bias', 'model.transformer.resblocks.8.ln_2.weight', 'model.transformer.resblocks.8.ln_2.bias', 'model.transformer.resblocks.9.attn.in_proj_weight', 'model.transformer.resblocks.9.attn.in_proj_bias', 'model.transformer.resblocks.9.attn.out_proj.weight', 'model.transformer.resblocks.9.attn.out_proj.bias', 'model.transformer.resblocks.9.ln_1.weight', 'model.transformer.resblocks.9.ln_1.bias', 'model.transformer.resblocks.9.mlp.c_fc.weight', 'model.transformer.resblocks.9.mlp.c_fc.bias', 'model.transformer.resblocks.9.mlp.c_proj.weight', 'model.transformer.resblocks.9.mlp.c_proj.bias', 'model.transformer.resblocks.9.ln_2.weight', 'model.transformer.resblocks.9.ln_2.bias', 'model.transformer.resblocks.10.attn.in_proj_weight', 'model.transformer.resblocks.10.attn.in_proj_bias', 'model.transformer.resblocks.10.attn.out_proj.weight', 'model.transformer.resblocks.10.attn.out_proj.bias', 'model.transformer.resblocks.10.ln_1.weight', 'model.transformer.resblocks.10.ln_1.bias', 'model.transformer.resblocks.10.mlp.c_fc.weight', 'model.transformer.resblocks.10.mlp.c_fc.bias', 'model.transformer.resblocks.10.mlp.c_proj.weight', 'model.transformer.resblocks.10.mlp.c_proj.bias', 'model.transformer.resblocks.10.ln_2.weight', 'model.transformer.resblocks.10.ln_2.bias', 'model.transformer.resblocks.11.attn.in_proj_weight', 'model.transformer.resblocks.11.attn.in_proj_bias', 'model.transformer.resblocks.11.attn.out_proj.weight', 'model.transformer.resblocks.11.attn.out_proj.bias', 'model.transformer.resblocks.11.ln_1.weight', 'model.transformer.resblocks.11.ln_1.bias', 'model.transformer.resblocks.11.mlp.c_fc.weight', 'model.transformer.resblocks.11.mlp.c_fc.bias', 'model.transformer.resblocks.11.mlp.c_proj.weight', 'model.transformer.resblocks.11.mlp.c_proj.bias', 'model.transformer.resblocks.11.ln_2.weight', 'model.transformer.resblocks.11.ln_2.bias', 'model.ln_post.weight', 'model.ln_post.bias', 'model.transformation.norm1.weight', 'model.transformation.norm1.bias', 'model.transformation.attn.qkv.weight', 'model.transformation.attn.qkv.bias', 'model.transformation.attn.proj.weight', 'model.transformation.attn.proj.bias', 'model.transformation.norm2.weight', 'model.transformation.norm2.bias', 'model.transformation.mlp.fc1.weight', 'model.transformation.mlp.fc1.bias', 'model.transformation.mlp.fc2.weight', 'model.transformation.mlp.fc2.bias', 'model.head_resPrompt.weight', 'model.head_resPrompt.bias'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = torch.load(\"pretrained_video_models/transformation/1_8split_cat_prompt/hmdb/clip/8_joint_1p/results/checkpoint_epoch_00015.pyth\")\n",
    "m['model_state'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 64-bit ('adv_time')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d1575ee82d394047b56e81f71158463ee20d270d4f1227bc04bbf836c8363a91"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
